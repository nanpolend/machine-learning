{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNiWLuNrZuG1fIpmQebWQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanpolend/machine-learning/blob/master/Jane_street_ai%E6%AF%94%E8%B3%BD%E4%BB%A3%E7%A2%BCdeepseek%E7%89%88Gemini%E6%94%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Jane Street市場數據預測 - 繁體中文版\n",
        "結合深度學習與梯度提升樹的集成模型\n",
        "\"\"\"\n",
        "\n",
        "# 環境準備\n",
        "!pip install pandas scikit-learn xgboost tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, GaussianNoise, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# 數據預處理函數\n",
        "def 數據預處理(訓練數據, 測試數據=None):\n",
        "    \"\"\"\n",
        "    數據預處理流程:\n",
        "    1. 中位數填充缺失值\n",
        "    2. 標準化特徵\n",
        "    3. 生成目標變量(action)\n",
        "    \"\"\"\n",
        "    # 特徵列選取\n",
        "    特徵列 = [f'feature_{i}' for i in range(130)]\n",
        "\n",
        "    # 使用訓練數據的中位數填充缺失值\n",
        "    訓練數據填補 = 訓練數據.fillna(訓練數據.median())\n",
        "\n",
        "    if 測試數據 is not None:\n",
        "        測試數據填補 = 測試數據.fillna(訓練數據.median())  # 測試集使用訓練集統計量\n",
        "\n",
        "    # 標準化處理（僅擬合訓練集）\n",
        "    標準化器 = StandardScaler()\n",
        "    訓練數據填補[特徵列] = 標準化器.fit_transform(訓練數據填補[特徵列])\n",
        "\n",
        "    if 測試數據 is not None:\n",
        "        測試數據填補[特徵列] = 標準化器.transform(測試數據填補[特徵列])\n",
        "\n",
        "    # 目標變量生成（action=1當resp>0）\n",
        "    訓練數據填補['action'] = (訓練數據填補['resp'] > 0).astype(int)\n",
        "\n",
        "    if 測試數據 is not None:\n",
        "        測試數據填補['action'] = (測試數據填補['resp'] > 0).astype(int)\n",
        "\n",
        "    return 訓練數據填補, 測試數據填補, 特徵列\n",
        "\n",
        "# 建立自編碼器+MLP模型\n",
        "def 建立自編碼器_MLP(輸入維度):\n",
        "    \"\"\"\n",
        "    建立多任務學習模型:\n",
        "    1. 自編碼器部分用於特徵提取\n",
        "    2. MLP部分用於分類預測\n",
        "    \"\"\"\n",
        "    # 輸入層\n",
        "    輸入層 = Input(shape=(輸入維度,))\n",
        "\n",
        "    # 自編碼器部分（加入高斯噪聲防過擬合）\n",
        "    x = GaussianNoise(0.15)(輸入層)\n",
        "    x = Dense(128, activation='swish', kernel_initializer='he_normal')(x)\n",
        "    x = Dense(64, activation='swish')(x)\n",
        "    編碼層 = Dense(32, activation='swish', name='編碼輸出層')(x)\n",
        "    解碼層 = Dense(輸入維度, activation='linear')(編碼層)\n",
        "\n",
        "    # MLP分類部分（融合原始特徵與編碼特徵）\n",
        "    融合層 = Concatenate()([輸入層, 編碼層])\n",
        "    x = Dense(256, activation='swish')(融合層)\n",
        "    x = Dense(128, activation='swish')(x)\n",
        "    分類輸出 = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # 多任務模型定義\n",
        "    模型 = Model(inputs=輸入層, outputs=[解碼層, 分類輸出])\n",
        "    模型.compile(\n",
        "        loss={'dense_5': 'mse', 'dense_7': 'binary_crossentropy'},  # 需根據層名稱調整\n",
        "        loss_weights=[0.2, 0.8],  # 強化分類任務權重\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    )\n",
        "\n",
        "    return 模型\n",
        "\n",
        "# 主程序流程\n",
        "if __name__ == \"__main__\":\n",
        "    # 使用os.getcwd()獲取當前工作目錄，因為__file__在Jupyter Notebook中未定義\n",
        "    腳本目錄 = os.getcwd()\n",
        "\n",
        "    # 構建數據文件路徑\n",
        "    訓練數據路徑 = os.path.join(腳本目錄, 'train.csv')\n",
        "    測試數據路徑 = os.path.join(腳本目錄, 'test.csv')\n",
        "\n",
        "    # 載入訓練數據(train_df)和測試數據(test_df)\n",
        "    train_df = pd.read_csv(訓練數據路徑)\n",
        "    test_df = pd.read_csv(測試數據路徑)\n",
        "\n",
        "    # 數據預處理\n",
        "    訓練數據處理後, 測試數據處理後, 特徵列 = 數據預處理(train_df, test_df)\n",
        "\n",
        "    # 時間序列交叉驗證\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    測試預測列表 = []\n",
        "\n",
        "    for fold, (訓練索引, 驗證索引) in enumerate(tscv.split(訓練數據處理後)):\n",
        "        print(f'正在訓練第 {fold+1} 折')\n",
        "\n",
        "        # 數據分割\n",
        "        X訓練 = 訓練數據處理後.iloc[訓練索引][特徵列]\n",
        "        y訓練 = 訓練數據處理後.iloc[訓練索引]['action']\n",
        "        X驗證 = 訓練數據處理後.iloc[驗證索引][特徵列]\n",
        "        y驗證 = 訓練數據處理後.iloc[驗證索引]['action']\n",
        "\n",
        "        # 訓練自編碼器（添加Early Stopping）\n",
        "        自編碼器模型 = 建立自編碼器_MLP(len(特徵列))\n",
        "        訓練歷史 = 自編碼器模型.fit(\n",
        "            X訓練, [X訓練, y訓練],\n",
        "            validation_data=(X驗證, [X驗證, y驗證]),\n",
        "            epochs=50,\n",
        "            batch_size=2048,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # 提取編碼特徵（通過層名稱獲取）\n",
        "        編碼器 = Model(inputs=自編碼器模型.input, outputs=自編碼器模型.get_layer('編碼輸出層').output)\n",
        "        訓練編碼特徵 = 編碼器.predict(X訓練)\n",
        "        驗證編碼特徵 = 編碼器.predict(X驗證)\n",
        "\n",
        "        # 合併特徵（水平堆疊）\n",
        "        X訓練合併 = np.hstack([X訓練, 訓練編碼特徵])\n",
        "        X驗證合併 = np.hstack([X驗證, 驗證編碼特徵])\n",
        "\n",
        "        # 訓練XGBoost（啟用GPU加速）\n",
        "        xgb模型 = XGBClassifier(\n",
        "            n_estimators=800,\n",
        "            max_depth=7,\n",
        "            learning_rate=0.03,\n",
        "            subsample=0.85,\n",
        "            tree_method='gpu_hist',  # GPU加速\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "        xgb模型.fit(X訓練合併, y訓練)\n",
        "\n",
        "        # 預測測試集\n",
        "        測試編碼特徵 = 編碼器.predict(測試數據處理後[特徵列])\n",
        "        測試合併特徵 = np.hstack([測試數據處理後[特徵列], 測試編碼特徵])\n",
        "        測試預測列表.append(xgb模型.predict_proba(測試合併特徵)[:, 1])\n",
        "\n",
        "    # 生成提交文件（閾值可調）\n",
        "    最終預測 = np.mean(測試預測列表, axis=0)\n",
        "    提交文件 = 測試數據處理後[['row_id']].copy()\n",
        "    提交文件['action'] = (最終預測 > 0.5).astype(int)\n",
        "    提交文件.to_csv('submission_v2.csv', index=False)\n",
        "\n",
        "    # 效能監控\n",
        "    # 1. 繪製訓練曲線\n",
        "    plt.plot(訓練歷史.history['dense_7_loss'], label='訓練損失')  # 分類任務損失\n",
        "    plt.plot(訓練歷史.history['val_dense_7_loss'], label='驗證損失')\n",
        "    plt.legend()\n",
        "    plt.title('模型訓練曲線')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. 顯示XGBoost特徵重要性\n",
        "    from xgboost import plot_importance\n",
        "    plot_importance(xgb模型, max_num_features=15)\n",
        "    plt.title('特徵重要性')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. 預測分佈分析\n",
        "    import seaborn as sns\n",
        "    sns.histplot(最終預測, kde=True)\n",
        "    plt.xlabel('預測概率')\n",
        "    plt.title('測試集預測分佈')\n",
        "    plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tYsmR4bYnH_l",
        "outputId": "89d8cd26-58f7-467a-8923-4180f1420a65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-aa1f751a4379>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# 載入訓練數據(train_df)和測試數據(test_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m訓練數據路徑\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m測試數據路徑\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    }
  ]
}