{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 87793,
          "databundleVersionId": 12024591,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Kaggle: Stanford RNA 3D Foldindeepseek-chatgpt4o",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanpolend/machine-learning/blob/master/Kaggle_Stanford_RNA_3D_Foldindeepseek_chatgpt4o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Uj4mHqeUwi8q"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "stanford_rna_3d_folding_path = kagglehub.competition_download('stanford-rna-3d-folding')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "vrYGCq_twi8t"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-30T11:27:55.55407Z",
          "iopub.execute_input": "2025-04-30T11:27:55.554333Z",
          "iopub.status.idle": "2025-04-30T11:28:00.591415Z",
          "shell.execute_reply.started": "2025-04-30T11:27:55.554315Z",
          "shell.execute_reply": "2025-04-30T11:28:00.590606Z"
        },
        "id": "kf-sfszNwi8t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 安装依赖（已写在代码开头注释中）\n",
        "!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "trusted": true,
        "id": "2qKmALbwwi8t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建虚拟环境\n",
        "python -m venv pytorch_env\n",
        "source pytorch_env/bin/activate  # Linux/Mac\n",
        "pytorch_env\\Scripts\\activate    # Windows\n",
        "\n",
        "# 安装依赖\n",
        "pip install -r requirements.txt  # 需创建包含依赖版本的文件"
      ],
      "metadata": {
        "trusted": true,
        "id": "AhlZk4SDwi8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard --logdir=runs"
      ],
      "metadata": {
        "trusted": true,
        "id": "3wcyQ0UKwi8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "flowchart LR\n",
        "    RawData --> Preprocess -->|轉換為 Tensor| DataLoader\n",
        "    DataLoader -->|輸入批次數據| Model\n",
        "    Model -->|預測結果| Loss[計算損失]\n",
        "    Loss -->|反向傳播| Optimizer[更新參數]\n",
        "    Optimizer -->|調整學習率| Scheduler\n",
        "    Scheduler -->|動態更新| Optimizer\n",
        "    Loss -->|記錄指標| TensorBoard\n",
        "    Model -->|保存最佳權重| Checkpoint"
      ],
      "metadata": {
        "id": "dhb5MmDxwi8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 环境设置 ==========\n",
        "# 安装依赖（若在 Colab 运行需先执行此命令）\n",
        "# !pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "# !pip install tensorboard\n",
        "\n",
        "# ========== 导入库 ==========\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "# ========== 超参数配置 ==========\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE = 128\n",
        "LR = 1e-3\n",
        "EPOCHS = 50\n",
        "PATIENCE = 5\n",
        "MODEL_SAVE_PATH = './best_model.pth'\n",
        "\n",
        "# ========== 数据增强与标准化 ==========\n",
        "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
        "])\n",
        "\n",
        "# ========== 反标准化转换 ==========\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-m/s for m, s in zip(CIFAR_MEAN, CIFAR_STD)],\n",
        "    std=[1/s for s in CIFAR_STD]\n",
        ")\n",
        "\n",
        "# ========== 数据加载 ==========\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=train_transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=test_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=2, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# ========== 模型定义 ==========\n",
        "class EnhancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*4*4, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = EnhancedCNN().to(DEVICE)\n",
        "\n",
        "# ========== 训练工具初始化 ==========\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "scaler = GradScaler()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# 记录模型结构图\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(DEVICE)\n",
        "writer.add_graph(model, dummy_input)\n",
        "\n",
        "# ========== 训练函数 ==========\n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autoc"
      ],
      "metadata": {
        "trusted": true,
        "id": "v8amPzLewi8v"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}