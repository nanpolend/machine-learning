{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5m0cTLAZbk8Z4KN1/qLSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanpolend/machine-learning/blob/master/%E5%88%A9%E7%94%A8ai%E9%A0%90%E8%A8%93%E7%B7%B4%E6%A8%A1%E5%9E%8B%E9%A0%90%E6%B8%AC%E9%BB%83%E9%87%91%E8%B5%B0%E5%8B%A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Google Colab é»„é‡‘ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ (ç¨³å®šç‰ˆ)\n",
        "\n",
        "# ====================\n",
        "# ç¯å¢ƒé…ç½® (å¿…é¡»é¦–å…ˆè¿è¡Œ)\n",
        "# ====================\n",
        "!pip install --force-reinstall --no-deps \\\n",
        "numpy==1.23.5 \\\n",
        "pandas==1.5.3 \\\n",
        "tensorflow==2.12.0 \\\n",
        "keras==2.12.0 \\\n",
        "xgboost==1.7.6 \\\n",
        "scikit-learn==1.2.2 \\\n",
        "pandas_ta==0.3.14b0 \\\n",
        "matplotlib==3.7.1 \\\n",
        "shap==0.44.1 \\\n",
        "requests==2.31.0\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # è‡ªåŠ¨é‡å¯è¿è¡Œæ—¶\n",
        "\n",
        "# ====================\n",
        "# æ­£å¼ä»£ç  (é‡å¯åè¿è¡Œ)\n",
        "# ====================\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "import shap\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# ====================\n",
        "# ç³»ç»Ÿé…ç½®\n",
        "# ====================\n",
        "class Config:\n",
        "    SYMBOL = \"XAUUSD\"\n",
        "    API_KEY = \"DEMO_KEY\"  # æ›¿æ¢ä¸ºçœŸå®å¯†é’¥\n",
        "    LOOKBACK = 30         # æ—¶é—´çª—å£é•¿åº¦\n",
        "    TRAIN_DAYS = 1000     # è®­ç»ƒæ•°æ®å¤©æ•°\n",
        "    FEATURES = ['RSI_14', 'MACD_12_26_9', 'BBU_20_2.0',\n",
        "               'BBL_20_2.0', 'EMA_50', 'volatility']\n",
        "    TARGET = 'close'\n",
        "\n",
        "# ====================\n",
        "# æ•°æ®ç®¡ç†æ¨¡å—\n",
        "# ====================\n",
        "class DataManager:\n",
        "    @staticmethod\n",
        "    def fetch_data(url, params, is_historical=True):\n",
        "        \"\"\"é€šç”¨æ•°æ®è·å–æ–¹æ³•\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                df = pd.DataFrame(data['data' if is_historical else [data])\n",
        "                return DataManager._process_data(df, is_historical)\n",
        "            return DataManager.generate_data(is_historical)\n",
        "        except:\n",
        "            return DataManager.generate_data(is_historical)\n",
        "\n",
        "    @staticmethod\n",
        "    def _process_data(df, is_historical):\n",
        "        \"\"\"æ•°æ®æ ¼å¼æ ‡å‡†åŒ–\"\"\"\n",
        "        df = df.rename(columns={\n",
        "            'Timestamp': 'timestamp', 'Open': 'open', 'High': 'high',\n",
        "            'Low': 'low', 'Close': 'close', 'Volume': 'volume'\n",
        "        })\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "        df.set_index('timestamp', inplace=True)\n",
        "        return df.astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_data(is_historical):\n",
        "        \"\"\"æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ\"\"\"\n",
        "        days = Config.TRAIN_DAYS if is_historical else 1\n",
        "        dates = pd.date_range(end=pd.Timestamp.now(), periods=days, freq='D')\n",
        "        base = 1800 + np.random.normal(0, 50, days).cumsum()\n",
        "        return pd.DataFrame({\n",
        "            'open': base + np.random.randint(-20, 20, days),\n",
        "            'high': base + np.random.randint(0, 30, days),\n",
        "            'low': base - np.random.randint(0, 30, days),\n",
        "            'close': base,\n",
        "            'volume': np.random.poisson(10000, days)\n",
        "        }, index=dates)\n",
        "\n",
        "# ====================\n",
        "# ç‰¹å¾å·¥ç¨‹æ¨¡å—\n",
        "# ====================\n",
        "class FeatureEngineer:\n",
        "    @staticmethod\n",
        "    def add_features(df):\n",
        "        \"\"\"åŠ¨æ€ç‰¹å¾ç”Ÿæˆ\"\"\"\n",
        "        # æŠ€æœ¯æŒ‡æ ‡\n",
        "        df.ta.rsi(length=14, append=True)\n",
        "        df.ta.macd(append=True)\n",
        "        df.ta.bbands(append=True)\n",
        "        df.ta.ema(length=50, append=True)\n",
        "\n",
        "        # è‡ªå®šä¹‰ç‰¹å¾\n",
        "        df['volatility'] = df['high'] - df['low']\n",
        "        df['momentum'] = df['close'].pct_change(5)\n",
        "        df['gap'] = df['open'] - df['close'].shift(1)\n",
        "\n",
        "        # æ¸…ç†æ— æ•ˆå€¼\n",
        "        return df.dropna().copy()\n",
        "\n",
        "# ====================\n",
        "# é¢„æµ‹æ¨¡å‹æ ¸å¿ƒ\n",
        "# ====================\n",
        "class GoldPredictor:\n",
        "    def __init__(self):\n",
        "        self.scaler = RobustScaler()\n",
        "        self.models = self._build_models()\n",
        "\n",
        "    def _build_models(self):\n",
        "        \"\"\"æ¨¡å‹æ¶æ„å®šä¹‰\"\"\"\n",
        "        # XGBoost\n",
        "        xgb = XGBRegressor(\n",
        "            objective='reg:squarederror',\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=5\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        lstm = Sequential([\n",
        "            LSTM(64, return_sequences=True,\n",
        "                 input_shape=(Config.LOOKBACK, len(Config.FEATURES))),\n",
        "            Dropout(0.3),\n",
        "            LSTM(32),\n",
        "            Dropout(0.3),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        return {'xgb': xgb, 'lstm': lstm}\n",
        "\n",
        "    def train(self, data):\n",
        "        \"\"\"è®­ç»ƒæµç¨‹\"\"\"\n",
        "        # ç‰¹å¾å·¥ç¨‹\n",
        "        df = FeatureEngineer.add_features(data)\n",
        "\n",
        "        # æ•°æ®æ ‡å‡†åŒ–\n",
        "        scaled = self.scaler.fit_transform(df[Config.FEATURES + [Config.TARGET]])\n",
        "        X, y = scaled[:, :-1], scaled[:, -1]\n",
        "\n",
        "        # XGBoostè®­ç»ƒ\n",
        "        self.models['xgb'].fit(X, y)\n",
        "\n",
        "        # LSTMæ•°æ®é‡æ„\n",
        "        X_lstm = np.array([\n",
        "            scaled[i-Config.LOOKBACK:i, :-1]\n",
        "            for i in range(Config.LOOKBACK, len(scaled))\n",
        "        ])\n",
        "        self.models['lstm'].fit(\n",
        "            X_lstm, y[Config.LOOKBACK:],\n",
        "            epochs=30,\n",
        "            batch_size=16,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"é›†æˆé¢„æµ‹\"\"\"\n",
        "        processed = FeatureEngineer.add_features(data)\n",
        "        scaled = self.scaler.transform(processed[Config.FEATURES + [Config.TARGET]])\n",
        "\n",
        "        # XGBoosté¢„æµ‹\n",
        "        xgb_pred = self.models['xgb'].predict(scaled[:, :-1])\n",
        "\n",
        "        # LSTMé¢„æµ‹\n",
        "        lstm_input = scaled[-Config.LOOKBACK:, :-1].reshape(1, Config.LOOKBACK, -1)\n",
        "        lstm_pred = self.models['lstm'].predict(lstm_input)\n",
        "\n",
        "        # ç»“æœèåˆ\n",
        "        return (xgb_pred[-1] * 0.6) + (lstm_pred[0, 0] * 0.4)\n",
        "\n",
        "    def explain(self, sample):\n",
        "        \"\"\"SHAPè§£é‡Š\"\"\"\n",
        "        explainer = shap.TreeExplainer(self.models['xgb'])\n",
        "        shap_values = explainer.shap_values(sample[Config.FEATURES])\n",
        "        shap.summary_plot(shap_values, Config.FEATURES, show=False)\n",
        "        plt.title('ç‰¹å¾å½±å“åŠ›åˆ†æ')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ====================\n",
        "# ä¸»æ§ç¨‹åº\n",
        "# ====================\n",
        "def main():\n",
        "    print(\"ğŸš€ ç³»ç»Ÿåˆå§‹åŒ–...\")\n",
        "\n",
        "    # æ•°æ®åŠ è½½\n",
        "    print(\"\\nğŸ“¥ æ•°æ®è·å–ä¸­...\")\n",
        "    historical_data = DataManager.fetch_data(\n",
        "        url=\"https://api.alltick.co/v1/history\",\n",
        "        params={\n",
        "            \"symbol\": Config.SYMBOL,\n",
        "            \"interval\": \"1d\",\n",
        "            \"apikey\": Config.API_KEY,\n",
        "            \"limit\": Config.TRAIN_DAYS\n",
        "        },\n",
        "        is_historical=True\n",
        "    )\n",
        "\n",
        "    # æ¨¡å‹è®­ç»ƒ\n",
        "    print(\"\\nğŸ¯ æ¨¡å‹è®­ç»ƒå¼€å§‹...\")\n",
        "    predictor = GoldPredictor()\n",
        "    try:\n",
        "        predictor.train(historical_data)\n",
        "        print(\"âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è®­ç»ƒå¤±è´¥: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # å®æ—¶é¢„æµ‹\n",
        "    print(\"\\nğŸ”® æ‰§è¡Œå®æ—¶é¢„æµ‹...\")\n",
        "    realtime_data = DataManager.fetch_data(\n",
        "        url=\"https://api.alltick.co/v1/quote\",\n",
        "        params={\"symbol\": Config.SYMBOL, \"apikey\": Config.API_KEY},\n",
        "        is_historical=False\n",
        "    )\n",
        "\n",
        "    if not realtime_data.empty:\n",
        "        try:\n",
        "            prediction = predictor.predict(\n",
        "                pd.concat([historical_data, realtime_data])\n",
        "            print(f\"\\nğŸ“Š é¢„æµ‹ä»·æ ¼: ${prediction:.2f}\")\n",
        "            print(f\"ğŸ•’ å®æ—¶ä»·æ ¼: ${realtime_data['close'].iloc[0]:.2f}\")\n",
        "            print(f\"ğŸ“ˆ å·®å¼‚å¹…åº¦: {abs(prediction - realtime_data['close'].iloc[0])/realtime_data['close'].iloc[0]*100:.2f}%\")\n",
        "\n",
        "            # ç‰¹å¾è§£é‡Š\n",
        "            predictor.explain(historical_data.sample(50))\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ é¢„æµ‹å¼‚å¸¸: {str(e)}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ å®æ—¶æ•°æ®è·å–å¤±è´¥\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MGHAogt__R0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ ¸å¿ƒæ¨¡å—åˆ’åˆ†\n",
        "+-------------------+\n",
        "|    DataManager    |  # ç»Ÿä¸€æ•°æ®æ¥å£\n",
        "+-------------------+\n",
        "|  FeatureEngineer  |  # åŠ¨æ€ç‰¹å¾ç”Ÿæˆ\n",
        "+-------------------+\n",
        "|   GoldPredictor   |  # æ¨¡å‹é›†æˆæ ¸å¿ƒ\n",
        "+-------------------+\n",
        "|      main()       |  # æµç¨‹æ§åˆ¶å™¨\n",
        "+-------------------+"
      ],
      "metadata": {
        "id": "eENGr3fY_TIr"
      }
    }
  ]
}